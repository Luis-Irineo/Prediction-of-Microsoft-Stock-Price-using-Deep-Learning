# -*- coding: utf-8 -*-
"""Predicción del Stock de  Microsoft vía Deeplearning por Datos Menores al N-ésimo Cuartil de Datos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nsm_lmYGKxQsKk_S7ywdUAf28xFwQ829
"""

# Si usas Google Colab
from google.colab import drive
drive.mount("/content/drive")
import os
os.chdir("/content/drive/MyDrive/Colab")

# Se importan las librerias
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar los datos de entrenamiento
train_data = pd.read_csv('Microsoft_Stock.csv', index_col='Date', parse_dates=True)

train_data.describe()

train_data.isna().sum()

sns.heatmap(train_data.corr(), annot=True)
plt.show()

sns.pairplot(train_data)
plt.show()

#train_data = np.round(train_data)
median = train_data.Open.median()
train_data_or = train_data[train_data['Open'] >=median ][['High','Low','Close']]

Quantiles = np.zeros(4)
for i in range(4):
  Quantiles[i] = train_data.Open.quantile(1/(i+1))

Numero_de_Epocas=5

import plotly.graph_objects as go
import pandas as pd
from sklearn.metrics import r2_score

#train_data = np.round(train_data)
for i in range(4):
  train_data_or = train_data[train_data['Open'] <= Quantiles[0] ][['High','Low','Close']]
  # Limpieza de datos
  train_data_or.replace(['---', '               ---', 'NaN', '', ' '], np.nan, inplace=True)
  imputer = SimpleImputer(strategy='mean')
  train_data_or = pd.DataFrame(imputer.fit_transform(train_data_or), columns=train_data_or.columns, index=train_data_or.index)
  # Escalado de datos
  scaler = MinMaxScaler(feature_range=(0, 1))    ###transforma los datos en ceros y uno
  train_scaled = scaler.fit_transform(train_data_or)
  # Crear secuencias de temporales, datos para el entrenamiento
  def create_dataset(dataset, look_back=60):
    X, Y = [], []
    for i in range(len(dataset) - look_back):
      a = dataset[i:(i + look_back), :]
      X.append(a)
      Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)
  X_train, y_train = create_dataset(train_scaled, 60)
  # Modelo LSTM
  model = Sequential([
      LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),  #return_sequences=True Devuelve salidas para cada paso (necesario para apilar LSTM)
      Dropout(0.2), #evita el sobreajuste , apaga aleatoriamete el 20% de las neuronas durante el entrenamiento#
      LSTM(100, return_sequences=False), #return sequences = False nos da la salid final y no toda la secuencia
      Dense(1) #capa final densa
      ])
  model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')
  # Entrenar el modelo
  model.fit(X_train, y_train, epochs=Numero_de_Epocas, batch_size=32, verbose=1)
  # Preparar los datos reales de 2024 para comparación
  real_data_2024 = pd.read_csv('Microsoft_Stock.csv', index_col='Date', parse_dates=True)
  real_data_2024 = real_data_2024[['High','Low','Close']]
  real_data_2024.replace(['---', '               ---', 'NaN', '', ' '], np.nan, inplace=True)
  real_data_2024 = pd.DataFrame(imputer.transform(real_data_2024), columns=real_data_2024.columns, index=real_data_2024.index)
  real_scaled = scaler.transform(real_data_2024)
  # Crear datos de entrada para predicción
  X_real, _ = create_dataset(np.vstack([train_scaled[-60:], real_scaled]), 60)
  # Predicciones
  predictions_scaled = model.predict(X_real)
  predictions = scaler.inverse_transform(np.concatenate([predictions_scaled, np.zeros((len(predictions_scaled), 2))], axis=1))[:, 0]
  # Datos reales para comparar
  real_demand = scaler.inverse_transform(real_scaled)[:, 0]
  # Graficar
  plt.figure(figsize=(14, 7))
  plt.plot(real_data_2024.index, real_demand, label='Datos Reales 2024', color='blue')
  plt.plot(real_data_2024.index[:len(predictions)], predictions, label='Predicciones 2024', color='red')
  plt.title(f'Predicciones vs Datos Reales 2015-2021 para el cuartil {i}')
  plt.xlabel('Fecha')
  plt.ylabel('Estimación de Venta del stock)')
  plt.legend()
  plt.grid(True)
  plt.show()
  # Asegurarse de que tanto 'real_demand' como 'predictions' tienen el mismo índice de tiempo
  real_demand = pd.Series(real_demand[:len(predictions)], index=real_data_2024.index[:len(predictions)])
  predictions_series = pd.Series(predictions, index=real_data_2024.index[:len(predictions)])
  # Crear una lista para almacenar los R-squared de cada mes
  r_squared_monthly = []
  # Agrupar los datos por mes y calcular el R-squared para cada mes
  for month, group in real_demand.groupby(real_demand.index.month):
    # Extraer las predicciones correspondientes para ese mes
    pred_for_month = predictions_series[group.index]
    # Calcular el R-squared
    r_squared = r2_score(group, pred_for_month)
    # Almacenar el resultado
    r_squared_monthly.append((month, r_squared))
    print(f'R-squared para el mes {month}: {r_squared}')